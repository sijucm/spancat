[paths]
train = "train.spacy"
dev = "dev.spacy"

[system]
gpu_allocator = "pytorch"
seed = 0

[nlp]
lang = "en"
pipeline = ["curated_transformer", "spancat"]
batch_size = 1000
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}

[components]

[components.curated_transformer]
factory = "curated_transformer"

[components.curated_transformer.model]
@architectures = "spacy-curated-transformers.RobertaTransformer.v1"
vocab_size = 50265
hidden_width = 768
attention_probs_dropout_prob = 0.1
hidden_act = "gelu"
hidden_dropout_prob = 0.1
intermediate_width = 3072
layer_norm_eps = 0.00001
max_position_embeddings = 514
model_max_length = 512
num_attention_heads = 12
num_hidden_layers = 12
padding_idx = 1
type_vocab_size = 1

[components.curated_transformer.model.with_spans]
@architectures = "spacy-curated-transformers.WithStridedSpans.v1"
stride = 104
window = 144
batch_size = 384

[components.spancat]
factory = "spancat"
scorer = {"@scorers":"spacy.spancat_scorer.v1"}
spans_key = "sc"
threshold = 0.5

[corpora]

[corpora.dev]
@readers = "spacy.Corpus.v1"
path = ${paths.dev}

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}

[training]
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dev_corpus = "corpora.dev"
train_corpus = "corpora.train"
dropout = 0.1
max_steps = 20000

[initialize]
vectors = null
init_tok2vec = null
vocab_data = null
lookups = null
before_init = null
after_init = null

# Proper initialization blocks
[initialize.components.curated_transformer]
piecer_loader = { "@model_loaders": "spacy-curated-transformers.HFPieceEncoderLoader.v1", "name": "roberta-base", "revision": "main" }

[initialize.components.spancat]
labels = ["YOUR_LABEL_HERE"]  # Replace with your actual labels

